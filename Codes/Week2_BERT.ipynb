{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2b38a7-81b6-473e-9399-31ca69619938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\miniconda3\\envs\\bert_wids\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Check device\u001b[39;00m\n\u001b[0;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import collections\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e25e48",
   "metadata": {},
   "source": [
    "Code block below handles the **Tokenizer** and **Vocabulary** logic. This is the first step in the NLP pipeline: converting raw human-readable text (strings) into machine-readable integers (indices).\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "1. **Tokenization:** The `tokenize` function splits a raw string (\"The cat sat.\") into a list of discrete units or tokens (`[\"the\", \"cat\", \"sat\", \".\"]`). This defines the fundamental unit of meaning for the model.\n",
    "2. **Vocabulary Building:** The `Vocab` class constructs a bijection (two-way mapping) between tokens and integer IDs.\n",
    "* **Forward Mapping (`token_to_idx`):** Converts \"apple\"  452. This is used to encode input text.\n",
    "* **Reverse Mapping (`idx_to_token`):** Converts 452  \"apple\". This is used to decode model predictions back into text.\n",
    "\n",
    "\n",
    "3. **Special Token Handling:** It automatically manages reserved tokens that are critical for BERT and other Transformer models:\n",
    "* `<pad>`: Used to make all sequences the same length.\n",
    "* `<mask>`: Used for the Masked Language Modeling task.\n",
    "* `<cls>`: The \"Classification\" token, used as the aggregate representation of the sequence.\n",
    "* `<sep>`: The \"Separator\" token, used to delimit sentences in pairs.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. Tokenization (`tokenize` function)**\n",
    "The code uses a simple whitespace-based tokenizer: `text.strip().lower().split()`.\n",
    "\n",
    "* **Lowercasing:** `lower()` converts \"The\" and \"the\" to the same token. This reduces the vocabulary size significantly, making the model more efficient, though it loses the distinction between proper nouns (e.g., \"Apple\" the company vs. \"apple\" the fruit).\n",
    "* **Splitting:** `split()` breaks the text on spaces.\n",
    "* *Limitation:* This simple tokenizer treats punctuation attached to words as part of the word (e.g., \"end.\" is different from \"end\"). Real-world BERT uses **WordPiece** tokenization, which splits \"playing\" into \"play\" + \"##ing\" to handle rare words and punctuation better.\n",
    "\n",
    "\n",
    "**B. The Vocabulary (`Vocab` class)**\n",
    "The vocabulary is the set of all unique tokens the model knows.\n",
    "\n",
    "* **Frequency Filtering (`min_freq=5`):** In any corpus, many words appear only once or twice (Zipf's Law). Learning embeddings for these extremely rare words is difficult because there are too few examples to update their weights meaningfully. Furthermore, they bloat the embedding matrix size.\n",
    "* **Theory:** By filtering out words with frequency , we reduce the parameter count and force the model to focus on generalizable patterns.\n",
    "* **The `<unk>` Token:** Words that are filtered out (or unseen during testing) are mapped to the special `<unk>` (Unknown) token. This ensures the model doesn't crash when it encounters a word it hasn't seen before; it just treats it as \"some generic unknown word.\"\n",
    "\n",
    "\n",
    "**C. The Padding Token (`<pad>`)**\n",
    "Deep learning models process data in batches (e.g., 32 sentences at a time). To stack 32 sentences into a single matrix, they must all have the same length.\n",
    "\n",
    "* **Theory:** If Sentence A has 10 words and Sentence B has 15, we append 5 `<pad>` tokens to Sentence A.\n",
    "* **Masking:** During training, we use a \"padding mask\" to tell the attention mechanism to *ignore* these `<pad>` tokens so they don't affect the meaning of the sentence.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Key Code Lines and Their Roles**\n",
    "\n",
    "**1. Reserved Tokens Initialization**\n",
    "\n",
    "```python\n",
    "self.idx_to_token = list(reserved_tokens)\n",
    "self.token_to_idx = {token: idx for idx, token in enumerate(reserved_tokens)}\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** This ensures that the special tokens (`<pad>`, `<mask>`, etc.) are always assigned the first few indices (0, 1, 2, 3). This is crucial because many models hardcode the index `0` for padding. If `<pad>` were assigned index 543, it would complicate the masking logic later.\n",
    "\n",
    "**2. Counting Token Frequencies**\n",
    "\n",
    "```python\n",
    "counter = collections.Counter()\n",
    "# ...\n",
    "counter.update(tokenize(sentence))\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** `collections.Counter` is a highly optimized hash map that counts unique elements. It iterates through the entire dataset to build the frequency distribution needed for filtering.\n",
    "\n",
    "**3. Frequency Filtering Logic**\n",
    "\n",
    "```python\n",
    "if freq >= min_freq and token not in self.token_to_idx:\n",
    "    self.idx_to_token.append(token)\n",
    "    self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** This is the gatekeeper. It checks two conditions:\n",
    "1. Is the word common enough? (`freq >= min_freq`)\n",
    "2. Is it already in the vocab? (To prevent duplicates if it was in `reserved_tokens`)\n",
    "Only if both pass is the word added to the official vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "**4. Handling Unknown Words (`__getitem__`)**\n",
    "\n",
    "```python\n",
    "return self.token_to_idx.get(tokens, self.unk)\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** This is the runtime lookup. When converting text to indices, `dict.get(key, default)` tries to find the word. If the word isn't found (because it was rare and filtered out), it returns `self.unk` (the index for `<unk>`). This makes the tokenizer robust to out-of-vocabulary (OOV) words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3247589-6ff8-4079-90d1-c6327f6e0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Simple word-level tokenization.\"\"\"\n",
    "    return text.strip().lower().split()\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary class for token-to-index mapping.\"\"\"\n",
    "    def __init__(self, sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>']):\n",
    "        self.idx_to_token = list(reserved_tokens)\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(reserved_tokens)}\n",
    "        \n",
    "        # Build counter for all tokens\n",
    "        counter = collections.Counter()\n",
    "        for sentence in sentences:\n",
    "            if isinstance(sentence, list): # Handle pre-tokenized input\n",
    "                counter.update(sentence)\n",
    "            else:\n",
    "                counter.update(tokenize(sentence))\n",
    "        \n",
    "        # Add tokens above min_freq\n",
    "        for token, freq in counter.items():\n",
    "            if freq >= min_freq and token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "        \n",
    "        self.pad = self.token_to_idx['<pad>']\n",
    "        self.unk = self.token_to_idx.get('<unk>', 0)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143f472",
   "metadata": {},
   "source": [
    "Below code block implements the **BERT Architecture**, which is fundamentally a bidirectional Transformer Encoder. It translates the theoretical design of BERT (Input Embeddings + Stacked Encoder Layers) into a PyTorch model.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "1. **`EncoderBlock`:** Represents a single layer of the Transformer. It contains the **Self-Attention** mechanism (to mix information between tokens) and a **Feed-Forward Network** (to process information individually), wrapped in residual connections and layer normalization.\n",
    "2. **`BERTEncoder`:** Stacks  of these blocks on top of each other. It also handles the complex input representation (Token + Segment + Position Embeddings) that is unique to BERT.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. The Encoder Block (`EncoderBlock`)**\n",
    "This is the standard building block of the Transformer (Vaswani et al., 2017).\n",
    "\n",
    "1. **Multi-Head Self-Attention:** Allows every token to look at every other token in the sequence. \"Multi-head\" means it runs  attention mechanisms in parallel, allowing the model to focus on different types of relationships (e.g., Head 1 focuses on syntax, Head 2 on semantics).\n",
    "2. **Add & Norm:**\n",
    "* **Residual Connection (Add):** . This allows gradients to flow through the network easily, solving the vanishing gradient problem in deep networks.\n",
    "* **Layer Normalization (Norm):** Normalizes the features for each token to have mean 0 and variance 1. This stabilizes training.\n",
    "\n",
    "\n",
    "3. **Position-wise Feed-Forward Network (FFN):** A simple MLP applied to every token independently. It acts as a key-value memory that stores knowledge learned from the pre-training data.\n",
    "\n",
    "**B. BERT's Input Embeddings**\n",
    "BERT's power comes from its ability to handle pairs of sentences.\n",
    "\n",
    "* **Token Embedding:** The standard vector lookup for words.\n",
    "* **Segment Embedding:** A learned vector that signals \"This token belongs to Sentence A\" vs \"Sentence B\". This allows BERT to distinguish between the premise and hypothesis in NLI tasks.\n",
    "* **Positional Embedding:** Unlike the original Transformer (which used fixed sine/cosine waves), BERT uses **learnable** positional embeddings. The model *learns* the best vector representation for \"Position 1\", \"Position 2\", etc.\n",
    "\n",
    "**C. The Padding Mask**\n",
    "Self-attention is . If we have a batch with sentences of length 10 and 50, we pad the short one with 40 zeros.\n",
    "\n",
    "* If we don't mask these zeros, the attention mechanism will treat them as valid context (like a period or a stop word).\n",
    "* **Masking Logic:** We create a boolean mask where `True` indicates padding. The attention mechanism sets the attention score for these positions to  (negative infinity). When Softmax is applied, , effectively forcing the model to ignore the padding.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Key Code Lines and Their Roles**\n",
    "\n",
    "**1. Multi-Head Attention**\n",
    "\n",
    "```python\n",
    "self.attention = nn.MultiheadAttention(..., batch_first=True)\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** This is the core engine. `batch_first=True` ensures the input tensors are `(Batch, Seq_Len, Dim)` rather than `(Seq_Len, Batch, Dim)`, which is more intuitive for NLP.\n",
    "\n",
    "**2. Learnable Positional Embeddings**\n",
    "\n",
    "```python\n",
    "self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** `nn.Parameter` tells PyTorch \"treat this tensor as a trainable weight.\" Unlike `nn.Embedding`, this isn't a lookup table; it's a raw tensor added directly to the input. The `1` in the shape allows broadcasting across the batch dimension.\n",
    "\n",
    "**3. Summing Embeddings**\n",
    "\n",
    "```python\n",
    "X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** This implements BERT's input formula: . The slicing `[:X.shape[1], :]` ensures that if the current batch has sequence length 20, we only add the first 20 positional vectors.\n",
    "\n",
    "**4. Padding Mask Construction**\n",
    "\n",
    "```python\n",
    "key_padding_mask[i, length:] = True\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** This loop builds the attention mask dynamically.\n",
    "* `valid_lens` tells us the real length of each sentence (e.g., `[5, 8]`).\n",
    "* For sentence `i`, indices from `length` to the end are set to `True` (ignore).\n",
    "* This mask is passed to `self.attention`, preventing the model from cheating or getting confused by empty padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d455baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer Encoder Block implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, num_heads, dropout, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        # Multi-head attention mechanism\n",
    "        # batch_first=True ensures input shape is (batch, seq, feature)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=num_hiddens, \n",
    "                                               num_heads=num_heads, \n",
    "                                               dropout=dropout, \n",
    "                                               batch_first=True)\n",
    "        # Add & Norm layer 1\n",
    "        self.addnorm1 = nn.LayerNorm(num_hiddens)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Position-wise Feed-Forward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(num_hiddens, ffn_num_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ffn_num_hiddens, num_hiddens)\n",
    "        )\n",
    "        # Add & Norm layer 2\n",
    "        self.addnorm2 = nn.LayerNorm(num_hiddens)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, key_padding_mask=None):\n",
    "        # Self-Attention\n",
    "        # key_padding_mask: (Batch, Seq_Len) where True indicates padding (ignore)\n",
    "        attn_output, _ = self.attention(X, X, X, key_padding_mask=key_padding_mask)\n",
    "        X = self.addnorm1(X + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed Forward\n",
    "        ffn_output = self.ffn(X)\n",
    "        X = self.addnorm2(X + self.dropout2(ffn_output))\n",
    "        return X\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT Encoder: Embeddings + Stack of Transformer Blocks.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\n",
    "                 num_blks, dropout, max_len=1000, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.ModuleList()\n",
    "        for _ in range(num_blks):\n",
    "            self.blks.append(EncoderBlock(num_hiddens, ffn_num_hiddens, num_heads, dropout))\n",
    "        \n",
    "        # Positional embedding is learnable in BERT\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # Add embeddings\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding[:, :X.shape[1], :]\n",
    "        \n",
    "        # Create mask for padding tokens\n",
    "        # 1 means ignore (pad), 0 means keep\n",
    "        key_padding_mask = torch.zeros(tokens.shape, dtype=torch.bool, device=tokens.device)\n",
    "        if valid_lens is not None:\n",
    "            for i, length in enumerate(valid_lens):\n",
    "                key_padding_mask[i, length:] = True\n",
    "                \n",
    "        for blk in self.blks:\n",
    "            X = blk(X, key_padding_mask=key_padding_mask)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf512a2e",
   "metadata": {},
   "source": [
    "Code block below implements the **Prediction Heads** that are attached to the core BERT Encoder. These heads are responsible for transforming the encoder's hidden states into actual predictions for the two self-supervised learning tasks: **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "1. **`MaskLM`:** Solves the \"Cloze\" task. It takes the hidden vectors corresponding to the masked positions and projects them back to the vocabulary size to predict the original word.\n",
    "2. **`NextSentencePred`:** Solves the binary classification task. It takes the hidden vector of the special `<cls>` token and predicts whether the second sentence follows the first.\n",
    "3. **`BERTModel`:** The master container. It combines the `BERTEncoder` (from the previous block) with these two task-specific heads into a single trainable `nn.Module`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. Masked Language Modeling (MLM) Head**\n",
    "The MLM head is essentially a classifier.\n",
    "\n",
    "* **Input:** Hidden state  for a masked token at position .\n",
    "* **Transformation:** In the original BERT paper, the authors apply a non-linear transformation before the final classifier:\n",
    "\n",
    "* **Output:** A probability distribution over the vocabulary  (e.g., size 30,000).\n",
    "\n",
    "\n",
    "* **Note:** The weights  are often tied (shared) with the input embedding matrix to save parameters, though in this simplified implementation, they are separate.\n",
    "\n",
    "**B. Next Sentence Prediction (NSP) Head**\n",
    "The NSP head is a binary classifier trained on the `[CLS]` token.\n",
    "\n",
    "* **The `[CLS]` Token:** BERT is designed such that the hidden state of the very first token () aggregates information from the entire input sequence.\n",
    "* **Pooling:** We often apply a `Tanh` activation to this pooled representation before the classifier.\n",
    "* **Output:** A binary score (IsNext vs. NotNext).\n",
    "\n",
    "**C. Gathering Strategy**\n",
    "BERT processes a batch of sequences, each of length .\n",
    "\n",
    "* The output of the encoder is a tensor of shape `(Batch, L, Hidden)`.\n",
    "* We only need to make predictions for the 15% of tokens that were masked.\n",
    "* Instead of running the classifier on *all*  tokens (which is wasteful), we use index selection (`masked_X = X[batch_idx, pred_positions]`) to extract only the vectors we care about.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Key Code Lines and Their Roles**\n",
    "\n",
    "**1. Advanced Indexing for MLM**\n",
    "\n",
    "```python\n",
    "batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "masked_X = X[batch_idx, pred_positions]\n",
    "\n",
    "```\n",
    "* **Role:** This extracts the relevant hidden states.\n",
    "* `pred_positions` contains the indices of the masked tokens (e.g., `[3, 8]` for row 0).\n",
    "* `X` is the full sequence output.\n",
    "* This advanced indexing grabs `X[0, 3]` and `X[0, 8]`, collapsing the 3D tensor into a 2D batch of features to be fed into the MLP.\n",
    "\n",
    "**2. The MLM MLP Architecture**\n",
    "\n",
    "```python\n",
    "self.mlp = nn.Sequential(nn.Linear(...), nn.ReLU(), nn.LayerNorm(...), nn.Linear(...))\n",
    "\n",
    "```\n",
    "* **Role:** This implements the specific architecture described in the BERT paper. The `LayerNorm` here is crucial for training stability. Note that while the original paper used GELU, this implementation uses `ReLU` for simplicity (standard PyTorch didn't have GELU until later versions).\n",
    "\n",
    "**3. The NSP Pooling Layer**\n",
    "\n",
    "```python\n",
    "self.hidden = nn.Sequential(nn.Linear(..., ...), nn.Tanh())\n",
    "\n",
    "```\n",
    "* **Role:** This is the \"pooler\" layer. It takes the raw `<cls>` vector and processes it. The `Tanh` activation is a specific design choice from the original BERT implementation, likely inherited from older LSTM-based classification heads.\n",
    "\n",
    "**4. Forward Pass Logic**\n",
    "\n",
    "```python\n",
    "if pred_positions is not None:\n",
    "    mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "\n",
    "```\n",
    "* **Role:** This conditional logic allows the model to be flexible. During **Pretraining**, we provide `pred_positions` to compute the MLM loss. During **Fine-tuning** (downstream tasks like Sentiment Analysis), we might only care about the NSP head (or a new custom head) and can skip the MLM computation entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335a351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"MLM Head: Predicts masked tokens.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_hiddens, num_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(num_hiddens),\n",
    "            nn.Linear(num_hiddens, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        # Extract features of tokens to be predicted\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size, device=X.device)\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        \n",
    "        # Gather the hidden states at the masked positions\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat\n",
    "\n",
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"NSP Head: Binary classification (IsNext vs NotNext).\"\"\"\n",
    "    def __init__(self, num_hiddens, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_hiddens, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is the output of the <cls> token\n",
    "        return self.output(X)\n",
    "\n",
    "class BERTModel(nn.Module):\n",
    "    \"\"\"Full BERT Model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                 num_heads, num_blks, dropout, max_len=1000):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\n",
    "                                   num_heads, num_blks, dropout, max_len)\n",
    "        self.hidden = nn.Sequential(nn.Linear(num_hiddens, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens)\n",
    "        self.nsp = NextSentencePred(num_hiddens)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        \n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "            \n",
    "        # Use <cls> token (index 0) for NSP\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a6342",
   "metadata": {},
   "source": [
    "Below Code block implements the complex logic required to generate training data for BERT's two pretraining tasks: **Next Sentence Prediction (NSP)** and **Masked Language Modeling (MLM)**. Unlike Word2Vec, which simply slides a window over text, BERT requires structured sentence pairs and a carefully orchestrated masking strategy.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "1. **NSP Data Generation:** Creates pairs of sentences `(A, B)` where `B` is either the actual next sentence (IsNext) or a random sentence (NotNext). This teaches BERT to understand long-range discourse and relationships between segments.\n",
    "2. **MLM Data Generation:** Takes a sequence of tokens and applies the \"Cloze\" task logic: randomly masking 15% of tokens so the model can learn bidirectional context.\n",
    "3. **Special Token Management:** Inserts the critical `<cls>` (start) and `<sep>` (separator) tokens that define BERT's input structure.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. The NSP Task (Inter-Sentence Coherence)**\n",
    "Standard language models (like GPT) treat text as a continuous stream. BERT, however, is often used for tasks like Question Answering (SQuAD) or Natural Language Inference (MNLI), where understanding the relationship between *two* pieces of text is crucial.\n",
    "\n",
    "* **Positive Example:** \"The man went to the store.\"  \"He bought a gallon of milk.\" (Label: IsNext)\n",
    "* **Negative Example:** \"The man went to the store.\"  \"Penguins are flightless birds.\" (Label: NotNext)\n",
    "* **Sampling:** The code ensures a 50/50 balance between these two types to prevent the model from biased learning.\n",
    "\n",
    "**B. The MLM Task (Bidirectional Context)**\n",
    "The `_replace_mlm_tokens` function implements the core innovation of BERT.\n",
    "\n",
    "* **The Problem:** In a deep bidirectional network, word  can \"see itself\" in later layers if we just train it to predict the next word.\n",
    "* **The Solution:** We physically remove or corrupt the input token so the model *must* rely on context.\n",
    "* **The 80-10-10 Rule:**\n",
    "* **80% `<mask>`:** The standard Cloze task.\n",
    "* **10% Random:** Forces the model to mistrust the input embedding at any position, ensuring it always checks the context to verify if the word \"makes sense\".\n",
    "* **10% Original:** Biases the model toward the correct word. Without this, the model might learn that \"everything masked is garbage\" and drift away from meaningful representations for non-masked words.\n",
    "\n",
    "\n",
    "\n",
    "**C. Structural Formatting**\n",
    "BERT inputs require a rigid structure: `[CLS] Sentence A [SEP] Sentence B [SEP]`.\n",
    "\n",
    "* **Segment IDs:** A vector of 0s and 1s `[0, 0, ..., 0, 1, 1, ..., 1]` is generated alongside the tokens. This tells the self-attention mechanism which sentence a token belongs to, allowing it to differentiate between the \"Premise\" and \"Hypothesis\".\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Key Code Lines and Their Roles**\n",
    "\n",
    "**1. Paragraph-Level Sampling**\n",
    "\n",
    "```python\n",
    "if random.random() < 0.5:\n",
    "    is_next = True\n",
    "else:\n",
    "    next_sentence = random.choice(random.choice(paragraphs))\n",
    "    is_next = False\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** This is the NSP logic. It flips a coin. If Heads, it takes `paragraph[i+1]` (the true next sentence). If Tails, it jumps to a random paragraph entirely to fetch a disconnected sentence. This ensures the \"NotNext\" examples are semantically distinct.\n",
    "\n",
    "**2. The 80-10-10 Masking Implementation**\n",
    "\n",
    "```python\n",
    "if random.random() < 0.8:\n",
    "    masked_token = '<mask>'\n",
    "else:\n",
    "    if random.random() < 0.5:\n",
    "        masked_token = tokens[mlm_pred_position] # 10% Original\n",
    "    else:\n",
    "        masked_token = random.choice(vocab.idx_to_token) # 10% Random\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** This nested if-else block strictly enforces the probability distribution required for robust BERT training. Note that `0.5` in the `else` block corresponds to 10% because it is 50% of the remaining 20%.\n",
    "\n",
    "**3. Special Token Exclusion**\n",
    "\n",
    "```python\n",
    "if token in ['<cls>', '<sep>']:\n",
    "    continue\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** We must never mask the special structural tokens. If we masked `<sep>`, the model would lose track of where sentence A ends and B begins, destroying the NSP task.\n",
    "\n",
    "**4. Filtering Short Sentences**\n",
    "\n",
    "```python\n",
    "if len(line) < 5: continue\n",
    "\n",
    "```\n",
    "\n",
    "* **Role:** Very short sentences (\"Yes.\", \"No.\") provide almost no context for learning. Training on them is inefficient and potentially noisy. This line acts as a simple heuristic data cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a27ba734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"Helper to add <cls>, <sep> and generate segment ids.\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments\n",
    "\n",
    "def _read_wiki(data_dir):\n",
    "    \"\"\"Reads wikitext.train.tokens and splits into paragraphs/sentences.\"\"\"\n",
    "    file_name = os.path.join(data_dir, 'wikitext.train.tokens') \n",
    "    # NOTE: If your file is named 'wiki.train.tokens', change the line above.\n",
    "    \n",
    "    if not os.path.exists(file_name):\n",
    "        print(f\"File not found: {file_name}\")\n",
    "        return []\n",
    "\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    paragraphs = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if len(line) < 5: \n",
    "            continue\n",
    "        # Split by ' . ' is specific to WikiText formatting\n",
    "        sentences = line.split(' . ')\n",
    "        paragraph = [s.strip().lower().split() for s in sentences if len(s.strip()) > 0]\n",
    "        if len(paragraph) >= 1:\n",
    "            paragraphs.append(paragraph)\n",
    "            \n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    \"\"\"Generates NSP label and pair.\"\"\"\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # Randomly select a different paragraph and sentence\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next\n",
    "\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    \"\"\"Process one paragraph to get NSP samples.\"\"\"\n",
    "    nsp_data_from_paragraph = []\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
    "            paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        \n",
    "        # +3 for <cls>, <sep>, <sep>\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "            \n",
    "        tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph\n",
    "\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    \"\"\"Masking logic for MLM.\"\"\"\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    \n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        \n",
    "        masked_token = None\n",
    "        # 80%: replace with <mask>\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            # 10%: keep original\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            # 10%: replace with random word\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "                \n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "        \n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    \"\"\"Generate MLM inputs and labels for a token sequence.\"\"\"\n",
    "    candidate_pred_positions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "        \n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "        \n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    \n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6081c41",
   "metadata": {},
   "source": [
    "Code block below implements the `WikiTextDataset` class, which is a PyTorch `Dataset` subclass. This class serves as the final bridge between the raw processed data (NSP pairs and MLM masking) and the model training loop. It ensures data is properly formatted, padded, and converted into tensors for batch processing.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "1. **Orchestration:** It coordinates the entire pipeline. It calls the sentence splitting logic (`_read_wiki`), builds the vocabulary (`Vocab`), generates NSP pairs (`_get_nsp_data_from_paragraph`), and applies MLM masking (`_get_mlm_data_from_tokens`).\n",
    "2. **Standardization (Padding):** Neural networks require fixed-size inputs. This class ensures every sequence is padded to `max_len`, adjusting all corresponding label and segment arrays to match.\n",
    "3. **Tensor Conversion:** It converts raw Python lists (integers) into PyTorch Tensors (`torch.long` or `torch.float32`), making them ready for GPU acceleration.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. Flattening for Vocabulary Construction**\n",
    "Before we can process any examples, we need a complete vocabulary of the dataset.\n",
    "\n",
    "* **Logic:** The dataset is initially a list of paragraphs, where each paragraph is a list of sentences. The list comprehension `[s for p in paragraphs for s in p]` flattens this nested structure into a single long list of sentences.\n",
    "* **Why:** The `Vocab` class needs to see every word in the corpus to calculate frequencies and assign indices correctly.\n",
    "\n",
    "**B. The Data Processing Pipeline**\n",
    "The `__init__` method runs the full preprocessing pipeline once during initialization:\n",
    "\n",
    "1. **NSP Generation:** It iterates through paragraphs to create `(Sentence A, Sentence B)` pairs with IsNext/NotNext labels.\n",
    "2. **MLM Masking:** For each pair, it tokenizes the text into integers and applies the 80-10-10 masking rule.\n",
    "3. **Result:** A list of `examples` where each example contains all the necessary inputs and labels for one training step.\n",
    "\n",
    "**C. Padding Strategy**\n",
    "The `max_len` parameter dictates the static size of the input tensors (e.g., 64 or 512).\n",
    "\n",
    "* **Token IDs:** Padded with `<pad>`.\n",
    "* **Segment IDs:** Padded with `0`.\n",
    "* **MLM Positions:** Padded with `0`. This is safe because...\n",
    "* **MLM Weights:** We create a weight vector `[1, 1, ..., 0, 0]`. During loss calculation, we multiply the loss by this weight vector. The `0` weights ensure that the model is not penalized for predicting the padded \"fake\" masks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Key Code Lines and Their Roles**\n",
    "\n",
    "**1. Flattening Logic**\n",
    "\n",
    "```python\n",
    "sentences = [s for p in paragraphs for s in p]\n",
    "self.vocab = Vocab(sentences)\n",
    "\n",
    "```\n",
    "* **Role:** This prepares the raw material for the vocabulary builder. Without flattening, the `Vocab` class (if not designed to handle nested lists) might fail or produce incorrect counts.\n",
    "\n",
    "**2. Dynamic Padding Calculation**\n",
    "\n",
    "```python\n",
    "valid_len = len(token_ids)\n",
    "pad_len = max_len - valid_len\n",
    "token_ids = token_ids + [self.vocab.pad] * pad_len\n",
    "\n",
    "```\n",
    "* **Role:** This ensures structural integrity. If a sentence has 10 tokens and `max_len` is 64, it appends 54 padding tokens. This allows us to stack thousands of diverse sentences into a single rectangular matrix.\n",
    "\n",
    "**3. MLM Weighting for Padding**\n",
    "\n",
    "```python\n",
    "mlm_weights = [1.0] * num_mlm + [0.0] * mlm_pad\n",
    "\n",
    "```\n",
    "* **Role:** This is critical for the loss function.\n",
    "* We padded the `pred_positions` and `mlm_pred_label_ids` arrays to a fixed size (e.g., 10 predictions per sequence).\n",
    "* If a short sentence only has 3 masked tokens, the remaining 7 slots are garbage.\n",
    "* The `mlm_weights` vector tells the loss function: \"Count the error for the first 3 predictions, but multiply the error for the last 7 by zero.\"\n",
    "\n",
    "**4. Tensor Conversion**\n",
    "\n",
    "```python\n",
    "self.all_data.append((torch.tensor(...), ...))\n",
    "\n",
    "```\n",
    "* **Role:** By converting to tensors immediately and storing them in RAM (`self.all_data`), we save CPU time during training. When the `DataLoader` requests batch 5, the tensors are already ready to be shipped to the GPU; we don't need to convert them on the fly every millisecond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a534fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # Flatten for vocab building\n",
    "        sentences = [s for p in paragraphs for s in p]\n",
    "        self.vocab = Vocab(sentences)\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(_get_nsp_data_from_paragraph(\n",
    "                paragraph, paragraphs, self.vocab, max_len))\n",
    "        \n",
    "        # Process MLM\n",
    "        self.all_data = []\n",
    "        for tokens, segments, is_next in examples:\n",
    "            token_ids, pred_positions, mlm_pred_label_ids = _get_mlm_data_from_tokens(tokens, self.vocab)\n",
    "            \n",
    "            # Padding\n",
    "            valid_len = len(token_ids)\n",
    "            pad_len = max_len - valid_len\n",
    "            token_ids = token_ids + [self.vocab.pad] * pad_len\n",
    "            segments = segments + [0] * pad_len\n",
    "            \n",
    "            # MLM padding\n",
    "            num_mlm = len(pred_positions)\n",
    "            max_mlm = round(max_len * 0.15)\n",
    "            mlm_pad = max_mlm - num_mlm\n",
    "            pred_positions = pred_positions + [0] * mlm_pad\n",
    "            mlm_weights = [1.0] * num_mlm + [0.0] * mlm_pad\n",
    "            mlm_pred_label_ids = mlm_pred_label_ids + [0] * mlm_pad\n",
    "            \n",
    "            self.all_data.append((\n",
    "                torch.tensor(token_ids, dtype=torch.long),\n",
    "                torch.tensor(segments, dtype=torch.long),\n",
    "                torch.tensor(valid_len, dtype=torch.long),\n",
    "                torch.tensor(pred_positions, dtype=torch.long),\n",
    "                torch.tensor(mlm_weights, dtype=torch.float32),\n",
    "                torch.tensor(mlm_pred_label_ids, dtype=torch.long),\n",
    "                torch.tensor(is_next, dtype=torch.long)\n",
    "            ))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.all_data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d5fb63",
   "metadata": {},
   "source": [
    "### **Training Loop** \n",
    "\n",
    "Code block below demonstates the actual training of the BERT model. It combines all the components defined previously—data loading, model architecture, loss calculation—and executes the optimization process over multiple steps. \n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "1. **Orchestration:** It loads the data into memory, initializes the model with specific hyperparameters (like hidden size and dropout), and sets up the optimizer.\n",
    "2. **Optimization Loop:** It iterates through the dataset in batches. For each batch, it computes the forward pass (predictions), calculates the composite loss (MLM + NSP), and performs backpropagation to update the model weights.\n",
    "3. **Visualization:** It tracks the loss over time and plots the learning curve, allowing us to verify if the model is actually learning or diverging.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. Hyperparameters and Architecture Config**\n",
    "The code initializes a \"Small BERT\".\n",
    "\n",
    "* **`num_hiddens=128`:** The vector dimension . Standard BERT-Base is 768.\n",
    "* **`num_heads=4`:** The number of attention heads. Standard BERT-Base is 12.\n",
    "* **`num_blks=2`:** The depth of the network (layers). Standard BERT-Base is 12.\n",
    "* **Theory:** We use a smaller model here so it can train on a CPU or a single GPU in a reasonable time (seconds vs days) for demonstration purposes. The underlying math remains identical to the full-scale BERT.\n",
    "\n",
    "**B. The Optimization Step**\n",
    "The core of learning happens inside the `while step < NUM_STEPS` loop.\n",
    "\n",
    "1. **Zero Grad:** `optimizer.zero_grad()` clears old gradients.\n",
    "2. **Forward:** The model processes the batch.\n",
    "3. **Backward:** `total_loss.backward()` computes gradients .\n",
    "4. **Step:** `optimizer.step()` updates weights .\n",
    "\n",
    "**C. The Composite Loss Function**\n",
    "BERT optimizes two objectives simultaneously:\n",
    "\n",
    "\n",
    "* **NSP Loss:** Simple binary cross-entropy.\n",
    "* **MLM Loss:** Weighted cross-entropy.\n",
    "* The raw output of `mlm_Y_hat` is `(Batch, Max_Len, Vocab)`.\n",
    "* We reshape it to `(Batch * Max_Len, Vocab)` to treat every token prediction as an independent classification problem.\n",
    "* Crucially, we multiply by `mlm_weights` to ensure we only count the error for the 15% masked tokens and ignore the padding/unmasked tokens.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Key Code Lines and Their Roles**\n",
    "\n",
    "**1. MLM Loss Normalization**\n",
    "\n",
    "```python\n",
    "mlm_l = (mlm_l * mlm_weights.reshape(-1)).sum() / (mlm_weights.sum() + 1e-8)\n",
    "\n",
    "```\n",
    "* **Role:** This line is mathematically dense.\n",
    "* `mlm_l`: Contains the loss for *every* token position (masked or not).\n",
    "* `mlm_weights`: Is 1.0 for masked tokens, 0.0 otherwise.\n",
    "* **Operation:** We zero out the loss for unmasked tokens. Then, we sum the remaining loss and divide by the *number of masked tokens* (`mlm_weights.sum()`). This gives the average loss per masked prediction, which is the correct metric to minimize.\n",
    "\n",
    "**2. Unpacking the Batch**\n",
    "\n",
    "```python\n",
    "(token_ids, segments, valid_lens, pred_positions, ...) = [x.to(device) for x in batch]\n",
    "\n",
    "```\n",
    "* **Role:** The `DataLoader` returns a tuple of tensors on the CPU. This list comprehension efficiently moves every single tensor to the GPU (`cuda`) in one line. Without this, the model (on GPU) would try to read data from CPU RAM, causing a runtime error.\n",
    "\n",
    "**3. Tracking Metrics**\n",
    "\n",
    "```python\n",
    "losses_mlm.append(mlm_l.item())\n",
    "\n",
    "```\n",
    "* **Role:** `.item()` detaches the loss value from the PyTorch computation graph and converts it to a standard Python float. If we just appended `mlm_l`, we would store the entire graph history for every step, leading to a massive memory leak that would crash the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb899911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and processing data...\n",
      "File not found: ./wikitext-2\\wikitext.train.tokens\n",
      "No data found. Please check the path.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 128\n",
    "NUM_STEPS = 500\n",
    "DATA_DIR = './wikitext-2' # Ensure this points to where your wikitext.train.tokens is\n",
    "\n",
    "# Load Data\n",
    "print(\"Reading and processing data...\")\n",
    "paragraphs = _read_wiki(DATA_DIR)\n",
    "if not paragraphs:\n",
    "    print(\"No data found. Please check the path.\")\n",
    "else:\n",
    "    dataset = WikiTextDataset(paragraphs, MAX_LEN)\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    print(f\"Vocab Size: {len(dataset.vocab)}\")\n",
    "    print(f\"Dataset Size: {len(dataset)}\")\n",
    "    \n",
    "    # Initialize Model\n",
    "    net = BERTModel(vocab_size=len(dataset.vocab), \n",
    "                    num_hiddens=128, \n",
    "                    ffn_num_hiddens=256, \n",
    "                    num_heads=4, \n",
    "                    num_blks=2, \n",
    "                    dropout=0.1, \n",
    "                    max_len=MAX_LEN)\n",
    "    \n",
    "    net.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Training Loop\n",
    "    net.train()\n",
    "    step = 0\n",
    "    losses_mlm = []\n",
    "    losses_nsp = []\n",
    "    \n",
    "    print(f\"Starting training on {device}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while step < NUM_STEPS:\n",
    "        for batch in train_loader:\n",
    "            if step >= NUM_STEPS: break\n",
    "            \n",
    "            # Unpack batch\n",
    "            (token_ids, segments, valid_lens, pred_positions, \n",
    "             mlm_weights, mlm_labels, nsp_labels) = [x.to(device) for x in batch]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            _, mlm_Y_hat, nsp_Y_hat = net(token_ids, segments, valid_lens.reshape(-1), pred_positions)\n",
    "            \n",
    "            # MLM Loss\n",
    "            mlm_l = loss_fn(mlm_Y_hat.reshape(-1, len(dataset.vocab)), mlm_labels.reshape(-1))\n",
    "            mlm_l = (mlm_l * mlm_weights.reshape(-1)).sum() / (mlm_weights.sum() + 1e-8)\n",
    "            \n",
    "            # NSP Loss\n",
    "            nsp_l = loss_fn(nsp_Y_hat, nsp_labels)\n",
    "            nsp_l = nsp_l.mean()\n",
    "            \n",
    "            # Total Loss\n",
    "            total_loss = mlm_l + nsp_l\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses_mlm.append(mlm_l.item())\n",
    "            losses_nsp.append(nsp_l.item())\n",
    "            \n",
    "            if (step + 1) % 10 == 0:\n",
    "                print(f\"Step {step+1}/{NUM_STEPS} | MLM Loss: {mlm_l.item():.4f} | NSP Loss: {nsp_l.item():.4f}\")\n",
    "            \n",
    "            step += 1\n",
    "            \n",
    "    print(f\"Training finished in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    # Plotting Results\n",
    "    plt.plot(losses_mlm, label='MLM Loss')\n",
    "    plt.plot(losses_nsp, label='NSP Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbce6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_wids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
