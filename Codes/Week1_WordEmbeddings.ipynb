{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00d5a39",
   "metadata": {},
   "source": [
    "Natural Language Processing: Pretraining Word Embeddings\n",
    "This script covers the implementation of concepts of word embeddings, specifically focusing on:\n",
    "1.  **Word2Vec Pretraining**: Implementing the Skip-Gram model with Negative Sampling from scratch.\n",
    "2.  **Subword Embeddings**: Implementing Byte Pair Encoding (BPE) to handle subword tokenization.\n",
    "3.  **Word Similarity and Analogy**: Using pretrained GloVe vectors to solve semantic analogy tasks.\n",
    "\n",
    "**Note:** All functions are defined from scratch using `torch` and standard Python libraries i.e. no external framework has been used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881de093",
   "metadata": {},
   "source": [
    "Below code block establishes the **infrastructure layer** for the entire NLP project. Before implementing complex models like Word2Vec or BERT, we need reliable tools to handle data acquisition, hardware acceleration, and performance monitoring.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "The code serves three critical functions necessary for any Deep Learning workflow:\n",
    "\n",
    "1.  **Data Acquisition:** It automates the fetching of specific datasets (Penn Tree Bank for training, GloVe for analogies) from remote servers. This ensures the code is reproducible on any machine.\n",
    "2.  **Hardware Abstraction:** It abstracts the check for GPU availability (`try_gpu`). Deep learning models rely on matrix multiplications that run significantly faster on GPUs (CUDA) than CPUs.\n",
    "3.  **Training Telemetry:** It provides tools (`Timer`, `Accumulator`) to measure how fast the model trains (tokens/sec) and to track the loss over time.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. Dataset Downloading (`download_url`, `download_extract`)**\n",
    "In NLP, datasets are often large compressed archives.\n",
    "\n",
    "  * **Streaming Downloads:** The code uses `requests.get(..., stream=True)`. This is crucial because datasets can be larger than available RAM. Streaming allows the file to be written to disk in chunks rather than loaded entirely into memory first.\n",
    "  * **Presets:** The `DATA_URLS` dictionary maps shorthand names ('ptb', 'glove') to specific AWS S3 URLs. This simplifies the user experience in later cells—instead of typing a long URL, you just request `'ptb'`.\n",
    "\n",
    "**B. Hardware Acceleration (`try_gpu`)**\n",
    "\n",
    "  * **Tensor Operations:** Neural networks involve massive matrix multiplications. On a CPU, these are done sequentially or with limited parallelism. GPUs have thousands of cores designed for parallel arithmetic.\n",
    "  * **Device Agnosticism:** This function allows the code to run anywhere. If a GPU is detected, it returns `cuda:0`; otherwise, it falls back to `cpu`. This prevents the code from crashing on non-GPU machines (like standard laptops).\n",
    "\n",
    "**C. Metric Tracking (`Accumulator`)**\n",
    "\n",
    "  * **The Stochastic Gradient Descent (SGD) Problem:** In SGD, we update weights after every small \"batch\" of data. To understand how the model is doing overall, we need to calculate the average loss over an entire \"epoch\" (one full pass through the data).\n",
    "  * **Accumulation:** The `Accumulator` class maintains a running total of multiple metrics simultaneously (e.g., `[total_loss, total_tokens_processed]`). At the end of an epoch, we calculate `Average Loss = Total Loss / Total Tokens`.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **3. Key Code Lines & Their Roles**\n",
    "\n",
    "**1. Streaming Download Logic**\n",
    "\n",
    "```python\n",
    "r = requests.get(url, stream=True)\n",
    "for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "    fd.write(chunk)\n",
    "```\n",
    "\n",
    "  * **Role:** This ensures efficient memory usage. By writing `chunk_size` bytes at a time, we can download gigabyte-sized datasets (like GloVe) without crashing the program due to memory overflow.\n",
    "\n",
    "**2. GPU Detection**\n",
    "\n",
    "```python\n",
    "if torch.cuda.device_count() >= i + 1:\n",
    "    return torch.device(f'cuda:{i}')\n",
    "```\n",
    "\n",
    "  * **Role:** This interacts with the NVIDIA driver via PyTorch. It checks if the requested GPU index (`i`) physically exists. This line is the gateway to accelerating training speed by 10x-50x.\n",
    "\n",
    "**3. The Accumulation Logic**\n",
    "\n",
    "```python\n",
    "self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "```\n",
    "\n",
    "  * **Role:** This Python list comprehension performs element-wise addition.\n",
    "      * If `self.data` is `[100.0, 50]` (current sums) and `args` is `[2.5, 10]` (new batch values), the new state becomes `[102.5, 60]`.\n",
    "      * This is used inside the training loop to update `sum_loss` and `num_tokens` in a single line of code, keeping the training loop clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe49fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Utilities and Helper Functions\n",
    "\n",
    "# We start by defining essential utility functions that are handy and prove \n",
    "# to be very useful. These tools are for downloading datasets, timing \n",
    "# execution, and tracking training metrics.\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "\n",
    "# Dataset Downloading Utilities \n",
    "\n",
    "def download_url(url, save_path, chunk_size=128):\n",
    "    \"\"\"Downloads a file from a URL to a local path with a progress stream.\"\"\"\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "\n",
    "def download_extract(name, folder=None):\n",
    "    \"\"\"\n",
    "    Download and extract a zip file from the D2L data repository.\n",
    "    Returns the directory path containing the extracted files.\n",
    "    \"\"\"\n",
    "    DATA_URLS = {\n",
    "        'ptb': 'http://d2l-data.s3-accelerate.amazonaws.com/ptb.zip',\n",
    "        'glove.6b.50d': 'http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip'\n",
    "    }\n",
    "    \n",
    "    if name not in DATA_URLS:\n",
    "        # Fallback or error handling if needed\n",
    "        print(f\"Warning: {name} not in preset URLs.\")\n",
    "        return name\n",
    "    \n",
    "    url = DATA_URLS[name]\n",
    "    fname = os.path.join('./data', name + \".zip\")\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    \n",
    "    # Download if not exists\n",
    "    if not os.path.exists(fname):\n",
    "        print(f\"Downloading {name} from {url}...\")\n",
    "        download_url(url, fname)\n",
    "    \n",
    "    # Extract\n",
    "    base_dir = os.path.dirname(fname)\n",
    "    data_dir, _ = os.path.splitext(fname)\n",
    "    if folder:\n",
    "        data_dir = os.path.join(base_dir, folder)\n",
    "        \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Extracting {fname}...\")\n",
    "        with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
    "            zip_ref.extractall(base_dir)\n",
    "            \n",
    "    return data_dir\n",
    "\n",
    "# Hardware and Training Utilities\n",
    "\n",
    "def try_gpu(i=0):\n",
    "    \"\"\"Return gpu(i) if exists, otherwise cpu().\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "\n",
    "class Timer:\n",
    "    \"\"\"Record multiple running times.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.times = []\n",
    "        self.start()\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"Start the timer.\"\"\"\n",
    "        self.tik = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"Stop the timer and record the time in a list.\"\"\"\n",
    "        self.times.append(time.time() - self.tik)\n",
    "        return self.times[-1]\n",
    "\n",
    "class Accumulator:\n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29647e68",
   "metadata": {},
   "source": [
    "### Dataset Processing and Subsampling\n",
    "\n",
    "### 1\\. Purpose and Function\n",
    "\n",
    "This code block is the **Data Preprocessing Pipeline**. Its primary goal is to convert human-readable text (strings) into machine-readable data (numerical indices) while optimizing the dataset for efficient training. It performs three critical tasks:\n",
    "\n",
    "1.  **Loading**: Reads the Penn Tree Bank (PTB) corpus and splits it into tokens (words).\n",
    "2.  **Indexing (Vocab)**: Creates a dictionary mapping every unique word to a unique integer ID ($0, 1, 2, ...$).\n",
    "3.  **Subsampling**: Removes extremely frequent words (like \"the\", \"a\", \"in\") to speed up training and focus the model on semantically rich words.\n",
    "\n",
    "### 2\\. Theoretical Explanation\n",
    "\n",
    "#### A. Vocabulary Construction\n",
    "\n",
    "Neural networks cannot process strings directly; they require numerical vectors. The `Vocab` class constructs a bijection (two-way mapping) between words and integers.\n",
    "\n",
    "  * **The Unknown Token (`<unk>`)**: In any real-world corpus, there will be rare words. To keep the model memory-efficient, we often discard words that appear fewer times than `min_freq`. These are mapped to a special token `<unk>` (index 0). This ensures the model can handle unseen words during inference.\n",
    "\n",
    "#### B. Subsampling (Zipf's Law)\n",
    "\n",
    "Natural language follows **Zipf's Law**, which states that the frequency of a word is inversely proportional to its rank.\n",
    "\n",
    "  * The most frequent words (stopwords like \"the\", \"is\", \"a\") appear millions of times but carry very little specific semantic meaning.\n",
    "  * Context windows like `(\"the\", \"apple\")` appear frequently but teach the model very little about the meaning of \"apple\" compared to `(\"eating\", \"apple\")`.\n",
    "\n",
    "**Subsampling** addresses this by probabilistically discarding words. The formula used is:\n",
    "$$P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right)$$\n",
    "Where:\n",
    "\n",
    "  * $f(w_i)$ is the frequency (count/total) of the word $w_i$.\n",
    "  * $t$ is a threshold hyperparameter (typically $10^{-4}$).\n",
    "\n",
    "If $f(w_i) > t$ (the word is frequent), the probability of discarding it increases. This balances the dataset, allowing the model to see more rare, information-rich words within the limited training epochs.\n",
    "\n",
    "### 3\\. Key Code Lines\n",
    "\n",
    "  * **`counter = collections.Counter(...)`**:\n",
    "\n",
    "      * **Role**: Computes the raw frequency of every word in the corpus. This statistic is the foundation for both Vocabulary building (filtering rare words) and Subsampling (filtering frequent words).\n",
    "\n",
    "  * **`self.token_to_idx = {token: idx ...}`**:\n",
    "\n",
    "      * **Role**: Creates the hash map that provides $O(1)$ lookups to convert any word string into its integer ID. This is critical for performance during the data loading phase.\n",
    "\n",
    "  * **`return (random.uniform(0, 1) < math.sqrt(1e-4 / counter[token] * num_tokens))`**:\n",
    "\n",
    "      * **Role**:\n",
    "\n",
    "This single line implements the **Subsampling Formula**.\n",
    "*  `counter[token] * num_tokens` approximates the relative frequency $f(w_i)$.\n",
    "*  `1e-4` is the threshold $t$.\n",
    "*  If the random number is *less* than the calculated threshold term, the function returns `True` (keep the word). If the word is very frequent, the threshold term becomes small, making it harder to satisfy the condition, thus dropping the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8c8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Dataset Processing (PTB)\n",
    "# We will use the Penn Tree Bank (PTB) dataset for training Word2Vec. \n",
    "# This section handles:\n",
    "# 1.  Reading: Loading the text data.\n",
    "# 2.  Vocabulary: Building a vocabulary mapping tokens to indices.\n",
    "# 3.  Subsampling: Removing frequent words (like \"the\", \"a\") to speed up training and improve quality.\n",
    "\n",
    "\n",
    "def read_ptb():\n",
    "    \"\"\"Load the PTB dataset into a list of text lines.\"\"\"\n",
    "    data_dir = download_extract('ptb')\n",
    "    # PTB training file location\n",
    "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
    "        raw_text = f.read()\n",
    "    return [line.split() for line in raw_text.split('\\n')]\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"Vocabulary for mapping text tokens to numerical indices.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        \n",
    "        # Flatten token list\n",
    "        counter = collections.Counter([token for line in tokens for token in line])\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # The list of unique tokens\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
    "        \n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "def subsample(sentences, vocab):\n",
    "    \"\"\"\n",
    "    Subsample high-frequency words.\n",
    "    Words are discarded with probability P(w) = 1 - sqrt(t / f(w)).\n",
    "    \"\"\"\n",
    "    # Exclude unknown tokens from subsampling logic\n",
    "    sentences = [[token for token in line if vocab[token] != vocab.unk]\n",
    "                 for line in sentences]\n",
    "    \n",
    "    counter = collections.Counter([token for line in sentences for token in line])\n",
    "    num_tokens = sum(counter.values())\n",
    "\n",
    "    def keep(token):\n",
    "        return (random.uniform(0, 1) < math.sqrt(1e-4 / counter[token] * num_tokens))\n",
    "\n",
    "    return ([[token for token in line if keep(token)] for line in sentences], counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a740c4",
   "metadata": {},
   "source": [
    "Belw code block implements the core data generation logic for the **Skip-Gram** model. While the previous block converted text to integers, this block transforms those integers into the actual **(input, target)** pairs required for supervised learning, specifically applying the **Negative Sampling** optimization strategy.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "The code serves three main purposes:\n",
    "\n",
    "1.  **Context Window Generation:** For every word in the text (the \"center\" word), it identifies which words surround it (the \"context\" words) to form positive training examples.\n",
    "2.  **Noise Distribution Calculation:** It calculates a specific probability distribution for sampling \"noise\" words, raising word frequencies to the power of 0.75.\n",
    "3.  **Negative Sampling:** For every positive pair (center, context), it generates $K$ \"negative\" words (noise) that *do not* appear in the context. This prepares the data for the binary classification objective used in approximate training.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. Skip-Gram Context Extraction (`get_centers_and_contexts`)**\n",
    "In the Skip-Gram model, the goal is to predict context words ($w_o$) given a center word ($w_c$).\n",
    "\n",
    "  * **The Sliding Window:** We slide a window over the text. The word in the middle is the center; words to the left and right are the context.\n",
    "  * **Dynamic Window Size:** Notice the code uses `random.randint(1, max_window_size)`. In the original Word2Vec paper, the window size isn't fixed. By randomizing the window size for each center word, the model effectively gives **higher weight to closer words** (which are always included) and lower weight to distant words (which are sometimes excluded).\n",
    "\n",
    "**B. Negative Sampling (NEG) Theory**\n",
    "Standard Softmax requires calculating the probability for *every* word in the vocabulary (often 50,000+) to normalize the result. This is computationally expensive.\n",
    "\n",
    "  * **The Solution:** Instead of a massive multi-class classification, we turn it into a **Binary Classification** problem.\n",
    "      * Is the pair `(word_A, word_B)` real? (Target = 1)\n",
    "      * Is the pair `(word_A, word_Random)` fake? (Target = 0)\n",
    "  * **The Dataset:** To train this, we need \"Negative Samples\"—words that *could* have appeared but didn't.\n",
    "\n",
    "**C. The 0.75 Power Heuristic (`get_negatives`)**\n",
    "When selecting noise words, we don't pick them uniformly (randomly) nor strictly by frequency. We use a specific distribution:\n",
    "$$P(w_i) = \\frac{f(w_i)^{0.75}}{\\sum_{j} f(w_j)^{0.75}}$$\n",
    "\n",
    "  * **Why 0.75?** This empirical trick dampens the dominance of extremely frequent words (like \"the\") and slightly boosts the probability of sampling rare words as negatives. This ensures the model learns to distinguish the center word not just from \"the\" (which is easy) but from other meaningful words (which is harder and more educational for the embeddings).\n",
    "\n",
    "-----\n",
    "\n",
    "#### **3. Key Code Lines & Their Roles**\n",
    "\n",
    "**1. Dynamic Context Window**\n",
    "\n",
    "```python\n",
    "window_size = random.randint(1, max_window_size)\n",
    "indices = list(range(max(0, i - window_size), min(len(line), i + 1 + window_size)))\n",
    "```\n",
    "\n",
    "  * **Role:** This implements the stochastic window size. If `max_window_size` is 5, sometimes the window is 1 (very local context), sometimes 5 (broad context). This implicitly re-weights semantic relationships based on distance.\n",
    "\n",
    "**2. The 0.75 Power Heuristic**\n",
    "\n",
    "```python\n",
    "sampling_weights = [counter[vocab.to_tokens(i)]**0.75 for i in range(1, len(vocab))]\n",
    "```\n",
    "\n",
    "  * **Role:** This is the exact implementation of the Word2Vec noise distribution formula. It transforms raw counts into the specific weights used for Negative Sampling.\n",
    "\n",
    "**3. Efficient Sampling Cache**\n",
    "\n",
    "```python\n",
    "self.candidates = random.choices(self.population, self.sampling_weights, k=10000)\n",
    "```\n",
    "\n",
    "  * **Role:** Generating random numbers is computationally slow in Python. Instead of calling `random.choices` (which calculates cumulative weights every time) for every single word, this class generates 10,000 choices at once and serves them from a cache. This provides a significant speedup during data loading.\n",
    "\n",
    "**4. Rejection Sampling**\n",
    "\n",
    "```python\n",
    "if neg not in contexts:\n",
    "    negatives.append(neg)\n",
    "```\n",
    "\n",
    "  * **Role:** This ensures data integrity. If we accidentally sample a \"noise\" word that actually appears in the true context window, we reject it. We must not teach the model that a word is \"negative\" (label 0) if it is actually present in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337d7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Context Extraction and Negative Sampling\n",
    "# To train Skip-Gram, we need to generate pairs of (center, context) words.\n",
    "# 1.  Context Extraction: For each center word, we select a window of surrounding words.\n",
    "# 2.  Negative Sampling: For every positive (center, context) pair, we sample K noise words that do not appear in the context.\n",
    "\n",
    "\n",
    "def get_centers_and_contexts(corpus, max_window_size):\n",
    "    \"\"\"Return center words and context words in skip-gram.\"\"\"\n",
    "    centers, contexts = [], []\n",
    "    for line in corpus:\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        for i in range(len(line)):\n",
    "            # Random window size between 1 and max_window_size\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, i - window_size),\n",
    "                                 min(len(line), i + 1 + window_size)))\n",
    "            indices.remove(i) # Exclude center word itself\n",
    "            contexts.append([line[idx] for idx in indices])\n",
    "    return centers, contexts\n",
    "\n",
    "class RandomGenerator:\n",
    "    \"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\n",
    "    def __init__(self, sampling_weights):\n",
    "        self.population = list(range(1, len(sampling_weights) + 1))\n",
    "        self.sampling_weights = sampling_weights\n",
    "        self.candidates = []\n",
    "        self.i = 0\n",
    "\n",
    "    def draw(self):\n",
    "        if self.i == len(self.candidates):\n",
    "            # Cache 10000 random choices for speed\n",
    "            self.candidates = random.choices(\n",
    "                self.population, self.sampling_weights, k=10000)\n",
    "            self.i = 0\n",
    "        self.i += 1\n",
    "        return self.candidates[self.i - 1]\n",
    "\n",
    "def get_negatives(all_contexts, vocab, counter, K):\n",
    "    \"\"\"Return noise words in negative sampling.\"\"\"\n",
    "    # Sampling probability proportional to frequency^0.75\n",
    "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75\n",
    "                        for i in range(1, len(vocab))]\n",
    "    \n",
    "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            neg = generator.draw()\n",
    "            # Noise word cannot be in the context\n",
    "            if neg not in contexts:\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd066d23",
   "metadata": {},
   "source": [
    "Below code block handles the **Data Loading and Batching** mechanics. Once we have our raw training examples (Center, Context, Negatives), we need to feed them into the neural network. However, neural networks require inputs to be structured as fixed-size Matrices (Tensors), but our text data is naturally irregular (variable lengths). This block bridges that gap.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "The code performs three crucial operations to prepare data for the GPU:\n",
    "\n",
    "1.  **Padding (`batchify`):** Since context windows vary in size (due to the random window effect), we pad shorter sequences with zeros so they can be stacked into a single rectangular matrix.\n",
    "2.  **Mask Generation:** It creates a \"mask\" matrix that tells the model which data points are real and which are just padding (to be ignored).\n",
    "3.  **Label Construction:** It creates the target labels for the binary classification task: `1` for context words (Positive) and `0` for noise words (Negative).\n",
    "\n",
    "-----\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. The Ragged Tensor Problem**\n",
    "In the previous step, we defined a random window size.\n",
    "\n",
    "  * Example 1: Center \"apple\", Context [\"fruit\", \"red\"] (Length 2)\n",
    "  * Example 2: Center \"run\", Context [\"fast\"] (Length 1)\n",
    "\n",
    "If we try to stack these into a matrix, the dimensions don't match. We cannot create a Tensor of shape `(2, variable)`.\n",
    "\n",
    "**B. Padding and Masking**\n",
    "To fix this, we find the maximum length in the batch (say, 5) and fill the empty spots with a dummy value (usually 0).\n",
    "\n",
    "  * Example 1 becomes: `[\"fruit\", \"red\", 0, 0, 0]`\n",
    "  * Example 2 becomes: `[\"fast\", 0, 0, 0, 0]`\n",
    "\n",
    "However, we must ensure the model **does not learn** from these zeros. If the model tries to predict that \"run\" is related to the padding token \"0\", it will ruin the embeddings.\n",
    "\n",
    "  * **The Mask:** We generate a binary tensor where `1` indicates real data and `0` indicates padding. The loss function uses this mask to mathematically cancel out any error coming from the padding positions.\n",
    "\n",
    "**C. Supervised Labels for Skip-Gram**\n",
    "The Skip-Gram with Negative Sampling objective is to distinguish true context words from noise.\n",
    "\n",
    "  * **Positives (Context):** Assigned Label **1**.\n",
    "  * **Negatives (Noise):** Assigned Label **0**.\n",
    "  * **Padding:** Assigned Label **0** (but masked out, so it affects nothing).\n",
    "\n",
    "-----\n",
    "\n",
    "#### **3. Key Code Lines & Their Roles**\n",
    "\n",
    "**1. Dynamic Batch Sizing**\n",
    "\n",
    "```python\n",
    "max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "```\n",
    "\n",
    "  * **Role:** This calculates the size of the tensor for *this specific batch*. Instead of padding everything to a global maximum (e.g., 512), which wastes memory, we only pad to the longest sequence in the current set of 512 examples. This is a standard optimization in NLP called **Dynamic Padding**.\n",
    "\n",
    "**2. Concatenation and Padding**\n",
    "\n",
    "```python\n",
    "contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "```\n",
    "\n",
    "  * **Role:** This creates the input vector for the \"Context\" side of the model. Notice how **Positives** (`context`) and **Negatives** (`negative`) are merged into a single list. This allows us to compute the scores for both types of words in a single matrix multiplication step, drastically improving GPU efficiency.\n",
    "\n",
    "**3. Label Construction**\n",
    "\n",
    "```python\n",
    "labels += [[1] * len(context) + [0] * (len(negative) + (max_len - cur_len))]\n",
    "```\n",
    "\n",
    "  * **Role:** This builds the \"Ground Truth\".\n",
    "      * `[1] * len(context)`: The model *should* predict these words are related.\n",
    "      * `[0] * len(negative)`: The model *should* predict these words are NOT related.\n",
    "      * The padding is also labeled 0, but the mask will ensure this is ignored.\n",
    "\n",
    "**4. The Collate Function**\n",
    "\n",
    "```python\n",
    "collate_fn=batchify\n",
    "```\n",
    "\n",
    "  * **Role:** Passed to the PyTorch `DataLoader`. This function overrides the default behavior (which assumes data is already uniform) and applies our custom padding logic every time the data loader fetches a new batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5183acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data Loading and Batching\n",
    "# We define the `PTBDataset` and a custom `batchify` function. This collate \n",
    "# function pads short contexts with zeros so that we can process data in \n",
    "# mini-batches.\n",
    "\n",
    "\n",
    "def batchify(data):\n",
    "    \"\"\"Return a minibatch of examples for skip-gram with negative sampling.\"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    \n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        # Concatenate context and noise words, then pad\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        # Mask: 1 for valid tokens, 0 for padding\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        # Labels: 1 for context (positive), 0 for noise (negative) and padding\n",
    "        labels += [[1] * len(context) + [0] * (len(negative) + (max_len - cur_len))]\n",
    "        \n",
    "    return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))\n",
    "\n",
    "class PTBDataset(data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
    "    \"\"\"Download the PTB dataset and then load it into memory.\"\"\"\n",
    "    sentences = read_ptb()\n",
    "    vocab = Vocab(sentences, min_freq=10)\n",
    "    subsampled, counter = subsample(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    \n",
    "    all_centers, all_contexts = get_centers_and_contexts(corpus, max_window_size)\n",
    "    all_negatives = get_negatives(all_contexts, vocab, counter, num_noise_words)\n",
    "    \n",
    "    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n",
    "    data_iter = data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                                collate_fn=batchify)\n",
    "    return data_iter, vocab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d47c26d",
   "metadata": {},
   "source": [
    "Below code block defines the **Neural Network Architecture** and the **Optimization Objective** (Loss Function). This is the \"brain\" of the Word2Vec project. It translates the mathematical formulas of Skip-Gram and Negative Sampling into executable PyTorch modules.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "1.  **`SkipGramModel`:** Defines the learnable parameters. It creates two distinct vector spaces (Center and Context). During the forward pass, it calculates the similarity (dot product) between a center word and a list of target words (both positive context and negative noise).\n",
    "2.  **`SigmoidBCELoss`:** Defines how \"wrong\" the model is. It treats the problem as **Binary Classification**. It forces the model to predict `1` for context words and `0` for noise words, while mathematically ignoring any padding added during batching.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. The Two-Embedding Paradigm**\n",
    "As discussed in the theory section, Word2Vec learns two vectors for every word $i$:\n",
    "\n",
    "1.  $\\mathbf{v}_i$ (when $i$ is a center word).\n",
    "2.  $\\mathbf{u}_i$ (when $i$ is a context word).\n",
    "\n",
    "The code implements this using two separate `nn.Embedding` layers. An embedding layer is essentially a lookup table that maps an integer index to a dense vector.\n",
    "\n",
    "  * The goal of training is to maximize the dot product $\\mathbf{u}_o^\\top \\mathbf{v}_c$ for real context words and minimize it for noise words.\n",
    "\n",
    "**B. Batch Matrix Multiplication (BMM)**\n",
    "In a single batch, we have $B$ examples. For each example, we have 1 center word and $L$ targets (a mix of context and noise).\n",
    "\n",
    "  * Center Vector shape: $(B, 1, D)$ where $D$ is embedding dimension.\n",
    "  * Context Vectors shape: $(B, L, D)$.\n",
    "    To compute the dot product of the center against *all* $L$ targets simultaneously for *all* $B$ examples, we use `torch.bmm` (Batch Matrix Multiplication).\n",
    "\n",
    "**C. Binary Cross Entropy with Logits**\n",
    "The Negative Sampling objective function is:\n",
    "$$J = - \\left( \\log \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c) + \\sum \\log \\sigma(-\\mathbf{u}_k^\\top \\mathbf{v}_c) \\right)$$\n",
    "This is mathematically identical to **Binary Cross Entropy (BCE)** loss, where the label $y=1$ for context and $y=0$ for noise.\n",
    "\n",
    "  * **\"With Logits\":** The `forward` method returns raw dot products (logits). We do not apply the Sigmoid function explicitly in the model. Instead, we use `binary_cross_entropy_with_logits`, which applies the Sigmoid internally. This is numerically more stable than applying Sigmoid then Log.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **3. Key Code Lines & Their Roles**\n",
    "\n",
    "**1. Defining the Learnable Parameters**\n",
    "\n",
    "```python\n",
    "self.center_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "self.context_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "```\n",
    "\n",
    "  * **Role:** These lines allocate the memory for the vectors. If vocab size is 10,000 and dimension is 100, each line creates a $10,000 \\times 100$ matrix of float weights. These are the $\\theta$ parameters we will optimize.\n",
    "\n",
    "**2. The Geometric Operation (Dot Product)**\n",
    "\n",
    "```python\n",
    "pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "```\n",
    "\n",
    "  * **Role:** This calculates the similarity scores.\n",
    "      * `v` has shape $(B, 1, D)$.\n",
    "      * `u` has shape $(B, L, D)$.\n",
    "      * `u.permute(0, 2, 1)` creates the transpose $(B, D, L)$.\n",
    "      * `bmm` performs $(B, 1, D) \\times (B, D, L) \\rightarrow (B, 1, L)$.\n",
    "      * The result is a score for every context/noise word relative to the center word.\n",
    "\n",
    "**3. Masked Loss Calculation**\n",
    "\n",
    "```python\n",
    "out = nn.functional.binary_cross_entropy_with_logits(inputs, target, weight=mask, ...)\n",
    "```\n",
    "\n",
    "  * **Role:** This calculates the loss but solves the **Padding Problem**.\n",
    "      * Recall that we padded short sequences with zeros. The model produces predictions for these zeros.\n",
    "      * The `mask` tensor (containing 1s for real data and 0s for padding) is passed to the `weight` argument.\n",
    "      * This multiplies the loss at padding positions by 0, effectively erasing them. Without this, the model would try to learn relationships between words and the padding token, destroying performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4504de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Skip-Gram Model and Loss\n",
    "# Here we implement:\n",
    "# 1.  **SkipGramModel**: A simple neural network that looks up embeddings for the center word and the context/noise words, then computes their dot product.\n",
    "# 2.  **SigmoidBCELoss**: Binary Cross-Entropy Loss with masking (to ignore padding).\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        # Center word embeddings (v)\n",
    "        self.center_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "        # Context word embeddings (u)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self, center, contexts_and_negatives):\n",
    "        # center shape: (batch_size, 1)\n",
    "        # contexts_and_negatives shape: (batch_size, max_len)\n",
    "        v = self.center_embeddings(center)\n",
    "        u = self.context_embeddings(contexts_and_negatives)\n",
    "        # Compute dot product between v and u\n",
    "        pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "        return pred\n",
    "\n",
    "class SigmoidBCELoss(nn.Module):\n",
    "    \"\"\"Binary Cross-Entropy Loss with Masking.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SigmoidBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask=None):\n",
    "        out = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, target, weight=mask, reduction=\"none\")\n",
    "        return out.mean(dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bb960",
   "metadata": {},
   "source": [
    "Belwo code blck implements the **Training Loop**. This is where the actual learnin occurs. It takes the model (initialized with random numbers) and the data (prepared in previous steps), and iteratively adjusts the model's weights to minimize the prediction error.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "1.  **Initialization:** It sets the model's initial state using **Xavier Initialization** to ensure training stability.\n",
    "2.  **Optimization:** It uses **Stochastic Gradient Descent (via Adam)** to update the embedding vectors $\\mathbf{v}$ and $\\mathbf{u}$ based on the calculated gradients.\n",
    "3.  **Loss Normalization:** It correctly scales the loss to account for the variable-length sequences created by the padding process, ensuring the model isn't penalized for \"predicting\" padding tokens.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. Weight Initialization (`xavier_uniform_`)**\n",
    "Neural networks are sensitive to initial conditions.\n",
    "\n",
    "  * If weights are too small, signals vanish (gradients $\\rightarrow$ 0).\n",
    "  * If weights are too large, signals explode (gradients $\\rightarrow \\infty$).\n",
    "  * **Xavier (Glorot) Initialization** draws weights from a distribution designed to keep the variance of activations and gradients roughly constant across layers. For embeddings, this ensures that the dot products $\\mathbf{u}^\\top \\mathbf{v}$ start in a reasonable range where the Sigmoid function has a non-zero gradient.\n",
    "\n",
    "**B. The Optimization Algorithm (Adam)**\n",
    "The code uses `torch.optim.Adam`.\n",
    "\n",
    "  * In Word2Vec, some words are very frequent (\"the\") and some are rare (\"abacus\").\n",
    "  * Standard SGD applies the same learning rate to all parameters.\n",
    "  * **Adam (Adaptive Moment Estimation)** adapts the learning rate for each parameter individually. It effectively allows rare words to take larger learning steps than frequent words, which speeds up convergence significantly for sparse data like text.\n",
    "\n",
    "**C. Backpropagation and Computational Graphs**\n",
    "The line `l.sum().backward()` triggers the chain rule of calculus.\n",
    "\n",
    "1.  PyTorch traverses the graph from the Loss back to the Embeddings.\n",
    "2.  It computes $\\frac{\\partial J}{\\partial \\mathbf{v}_c}$ and $\\frac{\\partial J}{\\partial \\mathbf{u}_o}$.\n",
    "3.  `optimizer.step()` subtracts these gradients from the current weights: $\\theta \\leftarrow \\theta - \\eta \\cdot \\nabla \\theta$.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **3. Key Code Lines & Their Roles**\n",
    "\n",
    "**1. Xavier Initialization**\n",
    "\n",
    "```python\n",
    "nn.init.xavier_uniform_(m.weight)\n",
    "```\n",
    "\n",
    "  * **Role:** Replaces the default PyTorch initialization (which is often just $\\mathcal{N}(0, 1)$ or uniform) with Xavier initialization. This is a critical \"best practice\" to prevent the model from getting stuck immediately at the start of training.\n",
    "\n",
    "**2. Loss Renormalization**\n",
    "\n",
    "```python\n",
    "l = (loss_fn(...) / mask.sum(axis=1) * mask.shape[1])\n",
    "```\n",
    "\n",
    "  * **Role:** This is a subtle but crucial mathematical correction.\n",
    "      * The `SigmoidBCELoss` calculates the mean loss over the *entire* row (including padding).\n",
    "      * Example: A row has 2 real words and 3 pads. `loss_fn` divides the total error by 5.\n",
    "      * However, we only want the average over the **2 real words**.\n",
    "      * **Math:** We multiply by 5 (`mask.shape[1]`) to get the total sum back, then divide by 2 (`mask.sum(axis=1)`) to get the correct average. Without this, the gradients would be diluted by the zeros from the padding.\n",
    "\n",
    "**3. Gradient Reset**\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "  * **Role:** PyTorch accumulates gradients by default (for RNNs etc.). In this training loop, we must clear the gradients from the previous batch before calculating the new ones; otherwise, the gradients would add up indefinitely, leading to a massive step size that causes the model to diverge (Loss $\\rightarrow$ NaN).\n",
    "\n",
    "**4. GPU Transfer**\n",
    "\n",
    "```python\n",
    "center, context_negative, mask, label = [data.to(device) for data in batch]\n",
    "```\n",
    "\n",
    "  * **Role:** Moving the data tensors to the GPU memory (`cuda:0`). If the Model is on the GPU but the Data is on the CPU, PyTorch will throw a runtime error. Both must reside on the same physical device to perform the matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a737415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Word2Vec Training ---\n",
      "Downloading ptb from http://d2l-data.s3-accelerate.amazonaws.com/ptb.zip...\n",
      "Extracting ./data\\ptb.zip...\n",
      "Training on cpu...\n",
      "Epoch 1, Loss 0.481, 13192.2 tokens/sec\n",
      "Epoch 2, Loss 0.427, 11001.8 tokens/sec\n",
      "Epoch 3, Loss 0.404, 11411.5 tokens/sec\n",
      "Epoch 4, Loss 0.380, 14056.0 tokens/sec\n",
      "Epoch 5, Loss 0.360, 14425.5 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "#  Training Word2Vec\n",
    "# The training loop iterates over the dataset, computes the loss, and updates the embedding weights.\n",
    "\n",
    "\n",
    "def train_word2vec(net, data_iter, lr, num_epochs, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Embedding:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_fn = SigmoidBCELoss()\n",
    "    \n",
    "    print(f\"Training on {device}...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = Timer()\n",
    "        metric = Accumulator(2) # Sum of loss, number of tokens\n",
    "        \n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            center, context_negative, mask, label = [\n",
    "                data.to(device) for data in batch]\n",
    "            \n",
    "            pred = net(center, context_negative)\n",
    "            l = (loss_fn(pred.reshape(label.shape).float(), label.float(), mask)\n",
    "                 / mask.sum(axis=1) * mask.shape[1])\n",
    "            \n",
    "            l.sum().backward()\n",
    "            optimizer.step()\n",
    "            metric.add(l.sum(), l.numel())\n",
    "            \n",
    "        print(f'Epoch {epoch + 1}, Loss {metric[0] / metric[1]:.3f}, '\n",
    "              f'{metric[1] / timer.stop():.1f} tokens/sec')\n",
    "\n",
    "# Run Training \n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Starting Word2Vec Training ---\")\n",
    "    batch_size, max_window_size, num_noise_words = 512, 5, 5\n",
    "    device = try_gpu()\n",
    "    data_iter, vocab = load_data_ptb(batch_size, max_window_size, num_noise_words)\n",
    "\n",
    "    embed_size = 100\n",
    "    net = SkipGramModel(len(vocab), embed_size)\n",
    "    train_word2vec(net, data_iter, lr=0.002, num_epochs=5, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0167aa64",
   "metadata": {},
   "source": [
    "Below code block implements **Byte Pair Encoding (BPE)**, a subword tokenization algorithm. In the previous blocks (Word2Vec), we treated words as atomic units. If a word wasn't in the vocabulary, it became `<unk>` (Unknown). BPE solves this by breaking words down into smaller, meaningful chunks (subwords), allowing the model to handle rare words and even words it has never seen before by constructing them from known parts.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "The code performs three distinct steps of the BPE algorithm:\n",
    "\n",
    "1.  **Statistics Collection (`get_max_freq_pair`):** It analyzes the corpus to find which pair of adjacent symbols (e.g., 'e' and 'r') appears most frequently.\n",
    "2.  **Vocabulary Update (`merge_symbols`):** It permanently merges the most frequent pair into a new, single symbol (e.g., 'er'). This iteratively builds a vocabulary of longer, meaningful subwords.\n",
    "3.  **Inference / Segmentation (`segment_BPE`):** It applies the learned merge rules to tokenize new, unseen words (e.g., splitting \"fatter\" into \"fat\" + \"ter\" based on known subwords).\n",
    "\n",
    "-----\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. The Out-Of-Vocabulary (OOV) Problem**\n",
    "Traditional models (like the Skip-Gram we just built) have a fixed vocabulary.\n",
    "\n",
    "  * **Training:** \"fast\", \"faster\", \"fastest\".\n",
    "  * **Test:** \"fasting\".\n",
    "  * **Result:** The model treats \"fasting\" as `<unk>`, losing all semantic information, even though it knows \"fast\".\n",
    "\n",
    "**B. Byte Pair Encoding (BPE) Algorithm**\n",
    "BPE was originally a data compression algorithm. In NLP, it works by iteratively merging frequent characters.\n",
    "\n",
    "1.  **Initialization:** Start with a vocabulary of all individual characters. Every word is a sequence of characters: `(\"f\", \"a\", \"s\", \"t\", \"e\", \"r\")`.\n",
    "2.  **Counting:** Count how often every adjacent pair of symbols occurs.\n",
    "      * ('f', 'a'): 5 times\n",
    "      * ('e', 'r'): 20 times\n",
    "3.  **Merging:** Merge the most frequent pair. 'e' and 'r' become the new symbol 'er'.\n",
    "      * Word becomes: `(\"f\", \"a\", \"s\", \"t\", \"er\")`.\n",
    "4.  **Iteration:** Repeat this process $N$ times. Eventually, common words become single symbols (\"fast\"), while rare words remain split (\"fast\", \"ing\").\n",
    "\n",
    "**C. Morphology Learning**\n",
    "Crucially, BPE automatically learns morphological structures without explicit linguistic rules.\n",
    "\n",
    "  * It learns that \"ing\", \"ed\", \"er\", and \"est\" are common suffixes because they appear frequently at the ends of many different words.\n",
    "  * This allows the model to understand \"un-friend-li-ness\" even if it has never seen the full word \"unfriendliness\" before.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **3. Key Code Lines & Their Roles**\n",
    "\n",
    "**1. Finding the Best Merge Candidate**\n",
    "\n",
    "```python\n",
    "pairs[symbols[i], symbols[i + 1]] += freq\n",
    "```\n",
    "\n",
    "  * **Role:** This loop scans through every word in the dictionary. It looks at every adjacent pair of symbols (e.g., 't', 'a') and adds the word's frequency to that pair's count. This identifies the global \"most popular\" combination.\n",
    "\n",
    "**2. Performing the Merge**\n",
    "\n",
    "```python\n",
    "new_token = token.replace(' '.join(max_freq_pair), ''.join(max_freq_pair))\n",
    "```\n",
    "\n",
    "  * **Role:** This performs the actual merge operation in the dictionary.\n",
    "      * If `max_freq_pair` is `('t', 'a')`.\n",
    "      * `' '.join(...)` creates the string `\"t a\"`.\n",
    "      * `''.join(...)` creates the string `\"ta\"`.\n",
    "      * The `replace` function updates the word structure: `\"t a l l\"` becomes `\"ta l l\"`.\n",
    "\n",
    "**3. Segmentation (Inference)**\n",
    "\n",
    "```python\n",
    "while start < len(token) and start < end:\n",
    "    if token[start: end] in symbols:\n",
    "        cur_output.append(token[start: end])\n",
    "```\n",
    "\n",
    "  * **Role:** This is a **Greedy Matching** algorithm used when processing new text.\n",
    "    1.  It tries to match the longest possible substring of the word against the known vocabulary (`symbols`).\n",
    "    2.  If it finds a match (e.g., \"tall\"), it adds it to the output.\n",
    "    3.  If not, it shrinks the window (`end -= 1`) and tries again.\n",
    "    4.  This ensures that \"tallest\" is tokenized as `[\"tall\", \"est\"]` rather than `[\"t\", \"a\", \"l\", \"l\", \"e\", \"s\", \"t\"]`, preserving meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f281fa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting BPE Demo ---\n",
      "Merge #1: ('t', 'a')\n",
      "Merge #2: ('ta', 'l')\n",
      "Merge #3: ('tal', 'l')\n",
      "Merge #4: ('f', 'a')\n",
      "Merge #5: ('fa', 's')\n",
      "Merge #6: ('fas', 't')\n",
      "Merge #7: ('e', 'r')\n",
      "Merge #8: ('er', '_')\n",
      "Merge #9: ('tall', '_')\n",
      "Merge #10: ('fast', '_')\n"
     ]
    }
   ],
   "source": [
    "#  Subword Embedding (Byte Pair Encoding)\n",
    "# As discussed in the book (Section 15.6), Byte Pair Encoding (BPE) allows us \n",
    "# to build a vocabulary of subword units. This helps in handling rare or \n",
    "# out-of-vocabulary words.\n",
    "\n",
    "\n",
    "def get_max_freq_pair(token_freqs):\n",
    "    \"\"\"Get the most frequent pair of consecutive symbols.\"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for token, freq in token_freqs.items():\n",
    "        symbols = token.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            # Key is tuple of two consecutive symbols\n",
    "            pairs[symbols[i], symbols[i + 1]] += freq\n",
    "    return max(pairs, key=pairs.get) # Key with max value\n",
    "\n",
    "def merge_symbols(max_freq_pair, token_freqs, symbols):\n",
    "    \"\"\"Merge the most frequent pair into a new symbol.\"\"\"\n",
    "    symbols.append(''.join(max_freq_pair))\n",
    "    new_token_freqs = dict()\n",
    "    for token, freq in token_freqs.items():\n",
    "        new_token = token.replace(' '.join(max_freq_pair),\n",
    "                                  ''.join(max_freq_pair))\n",
    "        new_token_freqs[new_token] = token_freqs[token]\n",
    "    return new_token_freqs\n",
    "\n",
    "def segment_BPE(tokens, symbols):\n",
    "    \"\"\"Segment new tokens using the learned BPE symbols.\"\"\"\n",
    "    outputs = []\n",
    "    for token in tokens:\n",
    "        start, end = 0, len(token)\n",
    "        cur_output = []\n",
    "        while start < len(token) and start < end:\n",
    "            if token[start: end] in symbols:\n",
    "                cur_output.append(token[start: end])\n",
    "                start = end\n",
    "                end = len(token)\n",
    "            else:\n",
    "                end -= 1\n",
    "                if start >= end:\n",
    "                    cur_output.append('<unk>')\n",
    "                    break\n",
    "        outputs.append(' '.join(cur_output))\n",
    "    return outputs\n",
    "\n",
    "# BPE Demo \n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Starting BPE Demo ---\")\n",
    "    symbols = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "               'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "               '_', '[UNK]']\n",
    "    raw_token_freqs = {'fast_': 4, 'faster_': 3, 'tall_': 5, 'taller_': 4}\n",
    "    token_freqs = {}\n",
    "    for token, freq in raw_token_freqs.items():\n",
    "        token_freqs[' '.join(list(token))] = raw_token_freqs[token]\n",
    "\n",
    "    num_merges = 10\n",
    "    for i in range(num_merges):\n",
    "        max_freq_pair = get_max_freq_pair(token_freqs)\n",
    "        token_freqs = merge_symbols(max_freq_pair, token_freqs, symbols)\n",
    "        print(f'Merge #{i + 1}:', max_freq_pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66c4b09",
   "metadata": {},
   "source": [
    "The final code block implements **Word Similarity** and **Word Analogy** tasks. While the previous blocks focused on *training* a model from scratch, this block focuses on *evaluating* and *using* embeddings. Because the small Penn Tree Bank dataset used earlier is too small to learn complex semantic relationships, this block loads **Pretrained GloVe Embeddings** (trained on billions of words) to demonstrate the true power of distributed representations.\n",
    "\n",
    "#### **1. Purpose and Function**\n",
    "\n",
    "The code performs three main functions:\n",
    "\n",
    "1.  **Loading Pretrained Vectors (`TokenEmbedding`):** It parses a large text file containing pre-computed vectors (GloVe) and loads them into memory as a lookup table.\n",
    "2.  **Semantic Search (`get_similar_tokens`):** It finds words that are semantically closest to a query word (e.g., \"chip\" $\\rightarrow$ \"microprocessor\").\n",
    "3.  **Analogical Reasoning (`get_analogy`):** It solves algebraic puzzles of the form \"A is to B as C is to ?\" using vector arithmetic.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **2. Detailed Theoretical Breakdown**\n",
    "\n",
    "**A. Pretrained Embeddings (Transfer Learning)**\n",
    "Training high-quality embeddings requires massive datasets (Wikipedia, Common Crawl) and significant compute power.\n",
    "\n",
    "  * **GloVe (Global Vectors):** A popular set of pretrained vectors. The filename `glove.6B.50d` indicates it was trained on **6 Billion** tokens and each word is represented by a **50-dimensional** vector.\n",
    "  * By loading these, we transfer the knowledge learned from the massive corpus to our local application.\n",
    "\n",
    "**B. Cosine Similarity (`knn`)**\n",
    "In the vector space, \"similarity\" is defined by the angle between two vectors, not the Euclidean distance.\n",
    "$$\\text{similarity}(\\mathbf{x}, \\mathbf{y}) = \\cos(\\theta) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}$$\n",
    "\n",
    "  * If the vectors point in the exact same direction, $\\cos(0) = 1$.\n",
    "  * If they are orthogonal (unrelated), $\\cos(90^\\circ) = 0$.\n",
    "  * If they point in opposite directions, $\\cos(180^\\circ) = -1$.\n",
    "\n",
    "**C. Word Analogy (Vector Arithmetic)**\n",
    "One of the most famous properties of word embeddings is that semantic relationships are encoded as linear offsets in the vector space.\n",
    "\n",
    "  * The relationship \"Man $\\to$ Woman\" is captured by the difference vector: $\\mathbf{v}_{woman} - \\mathbf{v}_{man}$.\n",
    "  * To find the analogue for \"Son\", we add this difference vector to the \"Son\" vector:\n",
    "    $$\\mathbf{v}_{target} \\approx \\mathbf{v}_{son} + (\\mathbf{v}_{woman} - \\mathbf{v}_{man})$$\n",
    "  * We then search the vocabulary for the word vector closest to this result. Ideally, it should be \"Daughter\".\n",
    "\n",
    "-----\n",
    "\n",
    "#### **3. Key Code Lines & Their Roles**\n",
    "\n",
    "**1. Efficient Cosine Calculation**\n",
    "\n",
    "```python\n",
    "cos = torch.mv(W, x.reshape(-1,)) / (\n",
    "    torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *\n",
    "    torch.sqrt((x * x).sum()))\n",
    "```\n",
    "\n",
    "  * **Role:** This calculates the cosine similarity between the query vector `x` and **every single word** in the vocabulary `W` simultaneously.\n",
    "      * `torch.mv(W, x)`: Matrix-Vector multiplication. Computes the dot product $\\mathbf{w}_i \\cdot \\mathbf{x}$ for all $i$.\n",
    "      * `torch.sum(W * W, axis=1)`: Computes $\\|\\mathbf{w}_i\\|^2$ (squared magnitude) for all vectors.\n",
    "      * `1e-9`: A tiny epsilon added to prevent division by zero errors.\n",
    "\n",
    "**2. The Analogy Arithmetic**\n",
    "\n",
    "```python\n",
    "x = vecs[1] - vecs[0] + vecs[2]\n",
    "```\n",
    "\n",
    "  * **Role:** This single line implements the algebraic reasoning logic.\n",
    "      * `vecs[0]`: Vector for word 'a' (e.g., \"man\").\n",
    "      * `vecs[1]`: Vector for word 'b' (e.g., \"woman\").\n",
    "      * `vecs[2]`: Vector for word 'c' (e.g., \"son\").\n",
    "      * It computes the hypothetical position of the answer vector in the 50-dimensional space.\n",
    "\n",
    "**3. Parsing the GloVe File**\n",
    "\n",
    "```python\n",
    "elems = line.rstrip().split(' ')\n",
    "token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "```\n",
    "\n",
    "  * **Role:** GloVe files are simple text files where each line looks like: `apple 0.45 -0.12 0.98 ...`.\n",
    "      * This code separates the string token (\"apple\") from its numerical components.\n",
    "      * It converts the string of numbers into a list of floats, which eventually becomes a Tensor row in the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99e2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Analogy/Similarity Demo ---\n",
      "\n",
      "Similar to 'chip':\n",
      "cosine sim=0.856: chips\n",
      "cosine sim=0.749: intel\n",
      "cosine sim=0.749: electronics\n",
      "\n",
      "Analogy: man -> woman :: son -> ?\n",
      "daughter\n"
     ]
    }
   ],
   "source": [
    "#  Word Similarity and Analogy\n",
    "# Implement the similarity and analogy tasks are just for seeing how above models and training actually work. We download \n",
    "# pretrained GloVe vectors for this, as the small PTB dataset used above is too small to learn high-quality semantic relationships.\n",
    "\n",
    "\n",
    "class TokenEmbedding:\n",
    "    \"\"\"Token Embedding Loader (GloVe).\"\"\"\n",
    "    def __init__(self, embedding_name):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
    "            embedding_name)\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self, embedding_name):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        # Download GloVe 50d\n",
    "        data_dir = download_extract(embedding_name)\n",
    "        \n",
    "        # We specifically look for vec.txt inside the extracted folder\n",
    "        # Note: The zip might extract to a folder or flat files.\n",
    "        # For safety, we search for the specific txt file.\n",
    "        file_path = os.path.join(data_dir, 'vec.txt')\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "                    \n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, torch.tensor(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        return self.idx_to_vec[torch.tensor(indices)]\n",
    "\n",
    "def knn(W, x, k):\n",
    "    \"\"\"Find k-nearest neighbors using cosine similarity.\"\"\"\n",
    "    # Add 1e-9 for numerical stability\n",
    "    cos = torch.mv(W, x.reshape(-1,)) / (\n",
    "        torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *\n",
    "        torch.sqrt((x * x).sum()))\n",
    "    _, topk = torch.topk(cos, k=k)\n",
    "    return topk, [cos[int(i)] for i in topk]\n",
    "\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    \"\"\"Print similar tokens.\"\"\"\n",
    "    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n",
    "    for i, c in zip(topk[1:], cos[1:]):\n",
    "        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')\n",
    "\n",
    "def get_analogy(token_a, token_b, token_c, embed):\n",
    "    \"\"\"Compute analogy: a is to b as c is to ?\"\"\"\n",
    "    vecs = embed[[token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = knn(embed.idx_to_vec, x, 1)\n",
    "    return embed.idx_to_token[int(topk[0])]\n",
    "\n",
    "# Run Analogy Demo (Downloads GloVe)\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Starting Analogy/Similarity Demo ---\")\n",
    "    # Note: This downloads ~160MB. \n",
    "    glove_embedding = TokenEmbedding('glove.6b.50d')\n",
    "\n",
    "    print(\"\\nSimilar to 'chip':\")\n",
    "    get_similar_tokens('chip', 3, glove_embedding)\n",
    "\n",
    "    print(\"\\nAnalogy: man -> woman :: son -> ?\")\n",
    "    print(get_analogy('man', 'woman', 'son', glove_embedding))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
